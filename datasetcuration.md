# Norwegian ASR Reading List – Data Curation, Weak Supervision & Dataset Quality

Reading list focused on large-scale data pipelines, weak/partial supervision, and quality audits for ASR. Emphasis on post‑Whisper supervised or semi-supervised datasets.

---

## 1. Anatomy of Industrial‑Scale Multilingual ASR (Universal‑1)  
**Ramirez et al., 2024 (AssemblyAI)**  
**Link:** https://arxiv.org/abs/2404.09841  

**Why it matters**

This report is an unusually detailed description of a production multilingual ASR system (Universal‑1). It lays out the full pipeline: supervised data curation, pseudo-labeling on massive unlabeled corpora, self-supervised pre‑training (BEST‑RQ on Conformer encoders), fine-tuning with RNN‑T, and long-form inference.  [oai_citation:10‡arXiv](https://arxiv.org/html/2404.09841v2?utm_source=chatgpt.com)  

The supervised set (~188k hours) is filtered by comparing human “gold” transcripts against ASR outputs, discarding examples when metrics like WER, deletion patterns, or language-ID consistency look suspicious. Pseudo-labeled data (1.62M hours) is generated by *two* independent ASR models; segments where the machine transcripts disagree beyond a 20% WER threshold are dropped, preventing the student from reinforcing teacher errors.

**What to focus on**

- Section 3.1: breakdown into unsupervised, supervised, and pseudo-labeled data and how each is filtered.  
- The two-model agreement filter for pseudo-labels (simple but very effective).  
- Sampling ratios between supervised and pseudo-labeled data during fine-tuning (1.5× oversampling of supervised).  
- Long-form inference pipeline: chunking with VAD, batched RNN‑T decoding, and timestamp merging.  

If you read only one data-pipeline paper, make it this one; it directly informs how you might scale Norwegian + neighboring languages using mixed-quality supervision.

---

## 2. Granary: Speech Recognition and Translation Dataset in 25 European Languages  
**Koluguri et al., 2025 (NVIDIA)**  
**Link:** https://arxiv.org/abs/2505.13404  

**Why it matters**

Granary is a large-scale multilingual ASR+AST dataset (25 European languages) built explicitly for training Canary and future Parakeet models. It’s particularly interesting because it’s *not* just a scrape-and-go dataset: the authors describe a multi-stage pseudo-labeling and cleaning pipeline tailored to long-form web audio.  [oai_citation:11‡arXiv](https://arxiv.org/abs/2505.13404)  

The pipeline includes: segmentation, a two-pass teacher inference scheme, hallucination filtering, and punctuation restoration. First, long recordings are segmented into manageable pieces and transcribed by a strong teacher model. A second pass checks for unstable or hallucinated predictions (e.g., repetitions, off-language output), and such segments are removed. Punctuation and casing are restored using LLM-based post-processing. For translation, they run a Euro-centric LLM to generate source–target pairs from trusted transcriptions and then apply additional filters (length ratios, language ID consistency).

**What to focus on**

- The pseudo-labeling pipeline design and runtime considerations (they emphasize that it processes huge volumes within hours).  
- Their hallucination and instability criteria—these are directly reusable when filtering Whisper-style outputs on web audio.  
- How they achieve “≈50% less data for similar performance” by aggressively filtering rather than just adding more hours.  

This paper is an excellent template if you plan to expand Norwegian speech via web-scale pseudo-labeling but need aggressive, automated quality control.

---

## 3. MOSEL: 950,000 Hours of Speech Data for Open‑Source Speech Foundation Model Training on EU Languages  
**Gaido et al., EMNLP 2024**  
**Link:** https://arxiv.org/abs/2410.01036  

**Why it matters**

MOSEL is a fully *license-clean* corpus: 950k hours of speech in the 24 official EU languages, collected exclusively from open-source-compliant datasets. About 441k hours of unlabeled VoxPopuli and LibriLight audio receive automatic transcripts from Whisper large‑v3, which the project releases under CC‑BY.  [oai_citation:12‡arXiv](https://arxiv.org/abs/2410.01036?utm_source=chatgpt.com)  

For your team, MOSEL is important in two ways: first, as a ready-made source of multilingual (including some Scandinavian) speech; second, as a worked example of how to track licensing, repackage heterogeneous corpora, and document pseudo-labeled data. The blog and paper together describe practical issues like splitting long LibriLight segments into ≤30s chunks to respect Whisper’s input window, and how to publish pseudo-labels in a reusable format.

**What to focus on**

- How they survey and combine existing labeled and unlabeled corpora under open licenses.  
- The Whisper-based pseudo-labeling setup and how they deal with length and domain mismatches.  
- Their stance on openness: releasing both raw audio links and pseudo-transcripts with clear licensing.  

Use MOSEL as inspiration for documenting and releasing your own Norwegian data in a way that others can legally reuse.

---

## 4. YODAS: YouTube‑Oriented Dataset for Audio and Speech  
**Li et al., ASRU 2023 / arXiv 2024**  
**Link:** https://arxiv.org/abs/2406.00899  

**Why it matters**

YODAS is a huge YouTube-based dataset: over 500k hours of speech in 100+ languages, with both labeled (manual or automatic subtitles) and unlabeled subsets. It is one of the main open web-scale sources feeding OWSM v4.  [oai_citation:13‡arXiv](https://arxiv.org/abs/2406.00899)  

The collection pipeline describes how to crawl videos, obtain subtitles, and separate supervised from unsupervised speech. YODAS emphasizes CC-licensed audio rather than just links, which is crucial for redistribution. The analysis sections highlight issues you will also see in your own semi-aligned data: mis-timed subtitles, non-speech segments, and label noise.

**What to focus on**

- The data collection strategy from YouTube, including filtering for license, duration, and language.  
- Distribution of labeled vs. unlabeled hours and how that impacts possible SSL + supervised recipes.  
- The ASR baselines on top-15 languages, which give a sense of “out-of-the-box” difficulty.  

Read this before/alongside OWSM v4, since OWSM’s cleaning pipeline is effectively a second pass over YODAS.

---

## 5. OWSM v4: Improving Open Whisper‑Style Speech Models via Data Scaling and Cleaning  
**Zhang et al., 2025 (ESPnet + collaborators)**  
**Link:** https://arxiv.org/abs/2506.00338  

**Why it matters**

OWSM v4 is a masterclass in large-scale *data cleaning* for multilingual ASR. Starting from raw YODAS (≈370k hours in 149 languages), they build a three-stage pipeline: (1) resegment long-form audio using a pre-trained OWSM‑CTC model and CTC segmentation; (2) filter by language ID consistency (fastText for text + ECAPA-TDNN for audio); and (3) filter by per-language CTC confidence quantiles. This shrinks YODAS down to ~166k high-quality hours in 75 languages.  [oai_citation:14‡arXiv](https://arxiv.org/pdf/2506.00338)  

They then train new OWSM v4 AED and CTC models (E‑Branchformer encoders + Transformer decoders) on the cleaned YODAS + earlier OWSM data (≈320k hours total). These models consistently beat previous OWSM versions and are competitive with MSS/MMS and Whisper‑large‑v3 on FLEURS, MLS, and long-form corpora.

**What to focus on**

- Figure 1 and Section 2.1:  
  - **Resegmentation:** using CTC segmentation to realign subtitles and cut long recordings into ≤30s chunks.  
  - **LID filtering:** keeping only samples where original label, text-LID, and audio-LID agree.  
  - **CTC-score filtering:** computing language-wise quantiles and discarding long-form segments whose sub-utterances fall in the worst θCTC fraction.  
- How they sweep the CTC threshold to balance remaining hours vs. WER, then fix θCTC=0.10.  
- The openness table: differentiating open-weights vs. fully open models with code, logs, and data.  

This is a very concrete recipe for cleaning your own semi-aligned Norwegian web data with off-the-shelf ASR + LID models.

---

## 6. Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition (OTC)  
**Gao et al., ASRU 2023**  
**Link:** https://arxiv.org/abs/2309.15796  

**Why it matters**

OTC (“Omni-Temporal Classification”) is designed for training ASR models on *non-verbatim* transcripts (think noisy subtitles or loosely edited captions). It’s a generalization of CTC in the Weighted FST framework: they extend the CTC training graph with a special “uncertainty” token (⋆), self-loops, and bypass arcs, allowing the model to align acoustics with ⋆ instead of wrong tokens. Penalties on ⋆ arcs ensure the model doesn’t collapse into predicting ⋆ everywhere.  [oai_citation:15‡arXiv](https://arxiv.org/abs/2309.15796?utm_source=chatgpt.com)  

This is directly useful if you want to exploit huge amounts of Norwegian (or related) speech with imperfect subtitles, where small word-level mismatches are common. Instead of discarding such data, OTC lets you keep it while softening the impact of errors during training.

**What to focus on**

- How they modify the CTC WFST graph (G(y) → G_otc(y)) with self-loops and bypass arcs labeled by ⋆.  
- How penalties on ⋆ are scheduled over epochs to avoid degenerate solutions.  
- Experiments showing robustness up to ~70% transcript error rates where vanilla CTC collapses.  

Combine this with your own heuristics for detecting *systematic* label errors (e.g., hallucinations) for a powerful semi-aligned training recipe.

---

## 7. Bypass Temporal Classification: Weakly Supervised ASR with Imperfect Transcripts (BTC)  
**Gao et al., Interspeech 2023**  
**Link:** https://arxiv.org/abs/2306.01031  

**Why it matters**

BTC is an earlier, simpler variant in the same family as OTC. It augments the CTC grammar graph with parallel “⋆ arcs” next to each word in the transcript. These arcs act as a garbage collector: if some word in the transcript is wrong or inserted, the model can align the corresponding acoustics to ⋆ instead of learning an incorrect mapping. A decaying penalty on ⋆ keeps the model from overusing it.  [oai_citation:16‡arXiv](https://arxiv.org/abs/2306.01031?utm_source=chatgpt.com)  

BTC specifically targets substitution and insertion errors, whereas OTC also addresses deletions. Conceptually, it’s easier to implement if you already have a WFST-based CTC pipeline and want a first step toward weak supervision.

**What to focus on**

- The BTC graph topology and how ⋆ arcs are attached in parallel to words.  
- The scheduling of the penalty λ over epochs to avoid “all ⋆” solutions.  
- Comparisons with standard CTC when training on noisy captions (e.g., mis-subtitled LibriVox/YouTube data).  

If you’re building your own training graphs (k2/icefall, etc.), BTC/OTC give you concrete modifications for tolerating subtitle noise instead of throwing data away.

---

## 8. Weakly Supervised Data Refinement and Flexible Sequence Compression for Efficient Thai LLM‑based ASR (EThai‑ASR)  
**Shao et al., Interspeech 2025**  
**Link:** https://arxiv.org/abs/2505.22063  

**Why it matters**

This paper is a focused case study on a single-language (Thai) LLM-based ASR system, but the *data refinement* ideas are general. They start from a large weakly supervised corpus and iteratively correct labels using the model itself (“self-evolving data refinement”). The model is a Zipformer encoder feeding a Thai LLM, with an additional sequence compression module to speed up decoding.  [oai_citation:17‡arXiv](https://arxiv.org/abs/2505.22063?utm_source=chatgpt.com)  

The refinement loop uses confidence and consistency checks to identify low-quality labels, updates them with model predictions, and repeats. This is effectively iterative pseudo-label cleaning: the better the model gets, the more reliable its feedback and the better the dataset.

**What to focus on**

- How they define weak label correction: which metrics they use to decide when to trust the model over the existing label.  
- The interaction between data refinement and model improvements across iterations.  
- The final performance vs. naive training on the original weak labels.  

This is a good pattern if you want to gradually upgrade a noisy Norwegian dataset *without* abandoning it or reannotating everything.

---

## 9. uDistil‑Whisper: Label‑Free Data Filtering for Knowledge Distillation in Low‑Data Regimes  
**Waheed et al., NAACL 2025**  
**Link:** https://arxiv.org/abs/2407.01257  

**Why it matters**

uDistil‑Whisper tackles a very practical problem: how do you filter pseudo-labeled data for distillation *without* ground‑truth references? The authors distill Whisper into smaller students for Arabic (MSA + dialects), but everything is applicable to Norwegian.  [oai_citation:18‡arXiv](https://arxiv.org/abs/2407.01257?utm_source=chatgpt.com)  

The key contribution is a suite of label-free quality signals for pseudo-labels:

- Proxy models (e.g., SeamlessM4T) to compare teacher outputs via WER between models.  
- Teacher uncertainty (entropy, geometric mean of per-token confidences).  
- Language-model plausibility (NLL from a text LLM like AceGPT).  
- Multimodal alignment scores (SONAR embeddings for speech vs. text).  
- Synthetic speech + PESQ similarity between input audio and TTS from the pseudo-label.  

They combine these signals to filter out dodgy pseudo-labels, then train small students that sometimes outperform the Whisper teacher.

**What to focus on**

- The definition and calibration of each label-free filtering signal.  
- How much data they discard at different thresholds vs. resulting WER and compute savings.  
- How well the distilled students generalize compared to the teacher across dialects and domains.  

If you consider distilling a Norwegian Whisper/Parakeet teacher into a lighter model, this gives you a toolbox for filtering training data without human labels.

---

## 10. Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning  
**Lau et al., ACL 2025**  
**Link:** https://arxiv.org/abs/2506.17525  

**Why it matters**

This paper is a deep audit of three widely-used multilingual speech datasets: Common Voice 17.0, FLEURS, and VoxPopuli. The authors show that, especially for low-resource languages, a large proportion of audio is non-speech or otherwise unusable, topic domains are heavily skewed (e.g., Wikipedia-style sentences for FLEURS), and speaker diversity is often extremely low (even down to a single speaker).  [oai_citation:19‡arXiv](https://arxiv.org/abs/2506.17525?utm_source=chatgpt.com)  

They distinguish micro-level issues (non-speech segments, very short utterances, repetitive machine-generated prompts, limited speakers) and macro-level issues (digraphia, diglossia, mismatched writing systems). For ASR users, the message is: if you treat “hours of audio” as your unit of measure, you can badly misjudge data balance and fairness.

**What to focus on**

- The methodology for auditing datasets:  
  - Using “words/transcribed tokens” instead of raw hours to quantify usable speech.  
  - Qualitative topic analyses with native speakers.  
  - Speaker statistics and their implications for bias and overfitting.  
- Concrete examples of problematic languages in Common Voice and FLEURS and how these issues could distort ASR evaluation.  

This paper is an excellent checklist for your own Norwegian corpus: it will push you to think beyond “number of hours” and incorporate sociolinguistic and domain considerations in your data curation and test-set design.
